Paper;CMA Name (A1);Report Date (A2) [automatically extracted from bibliography;Terminology (A3);Implemented? (A4);Model (A5);Relatives (A6);Innovations vs Predecessors (A7);Data Type (B1);Content (B2);Detail (B3);Specification (B4);Specifity Level (B5);Structuring (B6);Data Structure (B7);Framed Purpose (C1);Traget Issues (C2);Examples (C3);Processing Algorithms (C4);Test Scenarios (C5);Evaluation (C6)
Arcos2011a;AML11;;Inference Trace;implemented;Nelson Narens;;"adjust retrieval and reuse of knowledge items (""cases"") together; internal testing similar to simulation";symbolic;inference traces;low-level;;general;no;not specified;improve case-based reasoning performance;adjust case retrieval heuristics;equipment design;parameter tuning;equipment design for control of hydrogen sulfide (H2S);"field test on application for design of pollution control equipment; ablation study; addition of metareasoning showed improvement"
Brody2013a;BCP13;;Immediate Processual Self, Monitoring, autobiographical data, autobigraphical self, self-modelling;;Thick Time, Unified Model;;Thick-time representation of self;Symbolic;Logic rule application, time frame, semantic knowledge;;;;;Active Logic Knowledge Base;"Creation of an ""Immediate Processual Self""";;yes (vague);;;
Daglarli2020a;D20;;Replay memory, experience memory replay;yes;"Mapping to brain structures (prefrontal cortex)
Models of biological neural activation functions: Hodgin-Huxley, ion flow based, etc.
Models of larger scale patterns: Dynamic Neural Fields";- none, but related work mentions DAC-X (we don't seem to have this architecture in our list) which also insists on biological plausibility, and eBICA;"unclear; more explicitly models itself on brain areas?";"Sub-symbolic
(though there is a fuzzy cognitive map for if-then processing)";"activation states of artificial neural networks rather than explicit representations of facts about the environment;
however, also has a fuzzy cognitive map to implement rule-like reasoning, where the input-output statements may
explicitly point to some communicable feature of the environment";"unsure what the answer is here for neural activations; if-then rules in the cognitive map are fuzzy";mathematical formulas for neural dynamics and for the fuzzy cognitive map;very specific to the situation;latest interaction is available for the learning process, i.e. some temporary storage of neural activation states so that reinforcement learning can update the networks;"attractor network(s), working memory stored as weights in it
fuzzy cognitive map which links input features as detected by convolutional networks to some output features (e.g. other events or states that may be causally inferred from the known ones) or actions";"computational approximation of conciousness/awareness
spatio-temporal and emotional reasoning
""rational social interaction""";improvements in attention, decision-making, problem solving, HRI;solving sorting or prediction tasks in logic puzzles;"inference on artificial neural networks (feed forward through layers, feedback where the network has it as in the case of LSTM)
fuzzy inference in a cognitive map
networks trained with deep reinforcement learning with partially observable states
genetic algorithms (allegedly) to update fuzzy cognitive map
";"Wisconsin Card Sorting Test -- rules change occasionally, forcing test takers to relearn
Memory game (find matching cards)
Tic-tac-toe";"Experimental with a real robot
Experiment descriptions are very hard to understand but seem to be such that the same kind of tasks are performed with variations in the cognitive architecture (e.g. the source of a reward signal, or an extra optimization during learning). These do not seem to be ablation studies (""what would happen if we turned metacognition off"")
The experiments show that reward ad accuracy increase to a plateau with time spent interacting with a human on a game/task, with the higher plateau being the system with a more sophisticated optimization and reward source

Part of the experiment looks at performance under different emotional states, but it is not clear how these were emulated in the robot or whether they actually mean (for the robot) what is claimed they mean"
Helgason2013a;H13;;Introspection, Operational History, contextualized process performance history;yes (AREA);;;;Symbolic;"* goal, utility, process, durability
* abstracted patterns from detailed event descriptions";Abstracted patterns;Ontological Conceptualization, Syntactitc Pattern Grammar;;;;Awareness;Prioritizing Goals;;"* ""When a new goal is generated, the CPPH is consulted by seeking entries that have a pattern matching the goal. If matches are found, the processes corresponding to each entry are assigned an increase in activation according to the utility value of the entry. Additionally, the fact that specific entries were used for the purpose of achieving a specific goal by activating selected processes is recorded for the lifetime of the goal. This is necessary to properly update the PCCH in the event that the goal is successfully achieved, in which case the entries used to increase activation of selected processes should receive increases in their utility values – if and only if the process activated by the entry is part of the chain of execution that terminated in goal achievement- the process rather than having individual process contribution entries being created and treating the goal achievement as a fundamentally new one. When generalized entries are reinforced in this manner, the new utility value resulting from the latest goal achievement is added to the exponential moving average (EMA) that encodes the utility of the entry.""
* Huffman Compressing for Novelty
* ""predictions are generated on the basis of the operational history of the system. A prediction (conclusion) will have a more compact form than the sum of information used to generate it.
* ""predictions should improve over time as the operational history of the system grows and more evidence (positive and negative) for causal links accumulates""
* ""Bottom-up attention, as presented and discussed in chapters 4 and 6, requires methods to evaluate the novelty of new information in relation to prior operational history of the system. While these concepts are well understood in common language, some discussion of their meaning is in order in context of the present work. How do we determine and measure what is novel or unexpected for an AI system? It is clear such assessments must be made based on the prior experience of the system. Events that the system has experienced on previous occasions are considered less novel than events similar to ones having been experienced before. The most naïve approach to determine novelty would to evaluate new events by searching for identical events in the entire operating history of the system. This approach has functional and practical problems: Absolutely identical events are unlikely to occur frequently in the operation the system; the description of events will involve continuous values and even separate instances would have different values for time of occurrence. It would seem more desirable to base novelty determination of new events on their similarity, on the level of informational representation, with prior events. Furthermore, storing the entire continuously growing operating history of the system is not reasonable due to limitations of memory, and even without memory limitations the size of the operational history would eventually become so extreme as to render simple search highly resource intensive.""
* While this presents a sufficient method of relating new goals to processes responsible for achieving identical goals in the past, it is important to note that identical goals are unlikely to occur often in the operational history of an AGI system, as the specification of goals involves individual entities and/or continuous values. Furthermore, the CPPH is likely to grow extremely large in size over time as the entire operational of goal-driven AGI system revolves around achieving goals at various levels of abstraction. Functionality that enables access to the CPPH on the basis of similarity (as opposed to exact matching) and compresses the size of the data contained therein is desirable. The pattern matching described in 7.5.1 can be of use for this purpose, as the pattern matching functionality is fully applicable to goals, with these being special types of data items. If a set of goals contained in the CPPH share common features, a special process of generalization can be used to generate one pattern from all these goals. In this case, entries for the original goals can be collapsed – and potentially discarded - into one more general entry where the goal specification is the new pattern. This enables the con- Helgi Páll Helgason 117 tents of the PGGH to be accessed on a basis of similarity – where similarity of a new goal with previous entries can be determined on the simple basis of whether it matches an existing pattern or not. Furthermore, as numerous entries may be collapsed into single entries, the growth of the PGGH over time is significantly reduced. Generalization must be a continuously running maintenance function of the PGGH. The principle of collapsing two or more process-to-goal contribution entries is based on shared references to entities and attributes in the original entries.";;
Kennedy2010b;K10;;reasoning traces, monitoring;"no; position paper";Federated model;;distributed metacognition with a diversity of algorithms;;;;;;;;social context awareness;;;"* ""The simplified artificial immune system that we use is based on two 
algorithms: one for use in the training phase and another for the operational 
phase (when real intrusions can occur). 
During the training phase, a ""positive detector"" is generated for every 
new event record encountered and given a weight of 1. For every subsequent 
occurrence of the same event during the training phase, the weight of its 
detector is incremented. At the end of the training phase, the total weight can 
be expressed as a percentage, where 100% means that the event occurred on 
every training phase cycle and 0% means that it never occurred (for example, 
if a rule's conditions were never checked or its actions were never run). 
During the operational phase, if a positive detector fails to match something 
in the trace and the detector's weight is 100%, this is regarded as an 
""absence"" anomaly. We need only this simple version of the algorithm to 
satisfy our restricted requirement stated above (since the critical components 
must indicate that they are alive on each cycle). 
To detect more subtle anomalies (such as modification or resource 
diversion), it is possible to detect changes in the overall trace, while tolerating 
noise. There will be some mismatches for detectors whose weights are less 
than the total number of cycles (as activity in each cycle varies). The simplest 
method is to define a threshold above which the weighted sum of mismatches 
would be considered as an anomaly, as in Forrest et al. (1994). """;;
Kennedy2012a;K12;;Meta-reasoning, reasoning trace, episodic memory of mental events, audit trail;No;Nelson-Narens, Theory of Mind, Metacognition is necessary for prospection (How will I behave in that situation?);Polyscheme (predecessor);;Symbolic;"""
T1
• What did the agent know initially? What did it see? What information did it consider to be relevant?
• How certain was the agent about its subsequent inferences (if
any)?
T2:
• Current goal and the options that were being considered;
• How the options were evaluated (positively or negatively);
• Which option was chosen, and why?
T3
• History of top-down attention focus: things which were deliberately added to working memory.
• History of “salience” events: bottom-up emergence of ideas or
noticing of details.
T4
• History of salience events which were disruptive;
• History of changes in subjective difficulty of a task due to other
pressures;
""";"""The traces T1 and T2 represent mental states on a high level, and
do not include the computational “fine structure”. To determine B’s
experience, the agent needs to simulate what it means to “know” or
to “see” something. One solution is to provide a mapping from a
low level trace (the fine structure) to a high level mental concept or
process, which may itself be embedded in a high level trace (such
as T1). This originates from the agent’s own understanding of its
information processing. Therefore, we also need a process by which
the agent learns to understand its mental states (a self-model), since
we are aiming for some bottom-up development in the agent’s ability
to make ethical decisions""

""Mental concepts in T1 and T2 may be defined in terms of lower level
trace patterns, of which specific instances are actual histories. For
example, a trace pattern might define the concept of “knows about
x” as “repeatedly able to retrieve with certainty the details of x when
questioned”.""";;;;;Moral Turing Test;Reasoning about Caring, Prospection, Ethical agent;;"""There are different ways in which a meta-reasoning process can use
a reasoning trace in a cognitive architecture. The following are two
possibilities:
• Integrity-checking: Meta-reasoning component M1 monitors
object-level O1 and checks if the trace satisfies a required pattern. When inspecting T1, the meta-reasoner compares the actual
trace with what it ought to be. For example: where the assumptions
correct? Did it consider all the information? Did it miss out any
options when making a decision? This is approximately the approach taken in [12], which emphases distribution of meta-levels
to ensure that all reasoning processes are satisfying the requirements.
• Failure diagnosis: the meta-reasoning component checks if the
current trace matches a known pattern of reasoning failure. This
is the approach taken in [6], which uses a taxonomy of different
types of failure. An example failure type is “contradiction between
expected and actual observations”.
In both cases, a set of generic trace patterns is held in long-term (semantic) memory, while specific instances of traces (audit trails in
episodic memory) are matched against the patterns. This is how an
agent monitors the integrity of its reasoning or “makes sense” of its
experience, depending on the respective paradigm. Both of these approaches may be combined.""
";;
Kennedy2021a;K21;;Metacognitive Monitoring, trace;No;Behaviour Change Theory, Nelson Narens Model;;;Symbolic;"""History of mental events"", ""changes in attention focus, changes in beliefs and evaluations, changes in deliberation state (current goal and options, which option was selected and why)""";;;;;;"Identification of ""satisfactory"" reasoning processes";;Yes, but vague;;;
Kennedy2002a;KS02;;monitoring, trace of execution events, event records;Yes;Distributed Reflection;;Multiple Meta-Level;Symbolic;"* ""The trace is a sequence of event records for one cycle. Two types of event are 
recorded: beginning of condition-checking of a rule and beginning of rule 
tiring. For example, a trace that is produced in an agent's cycle t by its objectlevel is loaded into the agent's database in cycle / + I by its internal sensors 
(data access procedures). Thus, it can compare the sensed trace with the 
expected trace represented by the model. In accordance with Fig. lb, an 
agent's meta-level evaluates traces from two different sources, namely its own 
object-level and its neighbor's meta-level. The trace of its neighbor's metalevel is the one produced by the neighbor's last execution cycle. The cyclenumber (timestamp) is recorded along with the trace.""
* ""An event record is also a declarative statement which uniquely identifies 
a component, namely, a (ruleset, rule) pair along with what happened to it. An 
example of an event record is: ""Rule R of ruleset S of observed agent A had 
its conditions checked at cycle T"". Consequently, a whole trace for one agent cycle is a conjunction of statements that are true for that cycle. The resulting 
database may be used for high-level reasoning and diagnosis, as well as for 
low-level statistical pattern-matching used in existing artificial immune 
systems, such as in Forrest et al. (1994). EMERALD (Porras & Neumann, 
1997) is an example that combines both these levels. """;;;System specific;Sequence;;Artificial Immune System;;No;"* Signature-based Artificial Immune System
* Weighted Detectors (Basically learned Atomic Queries)
* Interleaved Training and Operational Phases";Well, kinda, but not quantitative but qualitative;
Kennedy2003a;KS02;;experience;;;;;;;;;;;;;;;;;
Kennedy2003b;KS02;;;;;;;;;;;;;;;;;;;
Canbaloglu2023b;MT21;;mental model adaptation, self modelling networks;simulation;"Nelson Narens Model
Koriat, A.: Metacognition and consciousness. 
In: Zelavo, P.D., Moscovitch, M., Thompson, E. (eds.) 
Cambridge Handbook of Consciousness. Cambridge University Press, New York (2007)

Sjöström, P.J., Rancz, E.A., Roth, A., Hausser, M.: Dendritic excitability and synaptic plasticity. 
Physiol. Rev. 88(769–840), 2008 (2008)

Robinson, B.L., Harper, N.S., McAlpine, D.: Meta-adaptation in the auditory midbrain under cortical 
influence. Nat. Commun. 7, 13442 (2016)

Abraham, W.C., Bear, M.F.: Metaplasticity: the plasticity of synaptic plasticity. Trends Neurosci. 
19(4), 126–130 (1996)

Magerl, W., Hansen, N., Treede, R.D., Klein, T.: The human pain system exhibits higher-order 
plasticity (metaplasticity). Neurobiol. Learn. Mem. 154, 112–120 (2018)";"Van Ments, L., Treur, J.: Reflections on dynamics, adaptation and control: 
a cognitive architecture for mental models. Cogn. Syst. Res. 70, 1–9 (2021)";"adding a meta layer to control an adaptation
(learning) process";hybrid or subsymbolic;activation values for network states/vertices, weights and update speed parameters. Unlike most connectionist approaches however, the network vertices are semantically interpreted;higher levels of self-modelling networks affect update speed parameters and connection weights;dynamics equations for the self-modelling network;"specific to components of the network; e.g.
the speed with which some particular state
responds to input changes";"self modelling network: an object-level network
that models some task, situation or context,
and self-modelling layers that model the dynamic
parameters of lower layers i.e. connection strengths
and update speed parameters";"unclear but something that can represent a graph
with weights on edges and activation values on
vertices";"control the adaptation of a mental model
of a task, situation or context based on
new information";"computationally model the variation in neuroplasiticity
that has been observed for human brains";"a communication scenario between
a few agents";Hebbian learning, simulation based on dynamics equations;simulation of a communication situation;not really: a simulation is performed but its results not compared with anything other than asserting that some qualitative behaviors emerged from the system
vanMents2021a;MT21;;static/dynamic, mental/world dimensions of world models;no;mental models;not given;n/a;symbolic or hybrid;higher-order relations (where at least one participant is a (reified) relation from the lower level);unclear (the relations used in their discussion could operate at any layer of abstraction in principle);unclear (the relations used in their discussion could operate at any layer of abstraction in principle);unclear (the relations used in their discussion could operate at any layer of abstraction in principle);unclear (the relations used in their discussion could operate at any layer of abstraction in principle);at least strong enough to represent and answer what knowledge graphs can;use of mental models: simulation, adaptation, control, higher-order relations;interactions between capabilities that involve mental models;none;not given;none;none
Castelfranchi2019a;Opinion paper;;self-representation, meta-representation of mental states, meta-examine reasoning;;;;;Symbolic;;;;;;;;;;;;
Kotseruba2020a;Overview Paper;;;;;;;;;;;;;;;;;;;
Anderson2007a;Overview Paper;;;;;;;;;;;;;;;;;;;
Langley2009a;Overview Paper;;;;;;;;;;;;;;;;;;;
